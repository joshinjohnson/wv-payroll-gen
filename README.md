# Payroll Generator

## API Endpoints (OpenAPI spec: [payroll.yaml](./openapi/payroll.yaml))
- /upload
- /report

## Steps to run the application
1. Make sure you've Docker and Docker-compose installed
2. Move to current project's directory
3. Copy the [.payroll.yaml](./config/.payroll.yaml) to your `$HOME/` location. This contains db setup required by code to connect to Postgres container
4. Build and start Docker environment: `make start-dev-env`
5. Test endpoints

## Examples
### Upload worklog csv file
curl -X POST -H "Authorization: Bearer <token_specified_in_payroll_config.yaml>" -H "Content-Type: multipart/form-data" -F "file=@/Users/josh/Documents/projects/wave/test/time-report-42.csv" http://localhost:8088/upload

### Generate payroll report
curl -H "Authorization: Bearer <token_specified_in_payroll_config.yaml>" http://localhost:8088/report

### Project structure
The database handling logic, api handlers and core payroll service are separated into their own packages, and uses dependency injection design pattern for better maintainability and reusability.

1. `cmd/` folder contains code to generate cli application
2. `pkg/db/` contains code to connect to Postgres DB. Initial db setup script is defined under pkg/db/scripts, so all the initial work group rate is set here.
3. `pkg/payroll/` contains the main logic for payroll generator
4. `handler/` contains api handler logic. server and types files are automatically generated by `goapi-gen` library based on OpenAPI specification defined under openapi/payroll.yaml
5. `docker/` contains docker compose file
6. `tests/` contains integration tests

---

### Answers
1. How did you test that your implementation was correct?
I made sure to add enough unit tests for each major function used, along with a basic integration test to check if the endpoints are working as expected. Finally, I tested all the user action sequence manually using CURL commands.

2. If this application was destined for a production environment, what would you add or change?
- More unit and integration tests to test unexpected cases
- Health checks for the api endpoints
- Adding error codes with response messages for debugging.
- Changing db handling code, would've been better to use an ORM library instead of writing it from scratch.
- Code currently supports 10mb max csv file size. If we set the maximum possible size for it, a single user can drain the servers resource, so its better to rate limit the upload endpoint.
- Pagination for /report endpoint.
- The time it takes to process each file is different because of file size and server capability, so its better to separate the client and server using a queue, so they don't have to depend on each other, and we can respond to clients faster.
- If we have a queue implemented, we can add more number of workers which can pull the files from queue and process it concurrently.
- Currently, the report is generated on each GET call, this can be separated into a asynchronous process when a user uploads.
- Since, reports wont change with time and assuming time log data will be huge, we can switch from sql to nosql db like Cassandra which is efficient for write-heavy system and time-series data, and cache reports for reducing db calls.

3. What compromises did you have to make as a result of the time constraints of this challenge?
- Limited testing
- Cleaning up code and better documentation
- Since I wrote db handling logic from scratch instead of using an ORM library, the transaction handling in service layer could've been improved. Since the implementation was little complex, it would've complicated the unit test logic as well, so I decided against it.